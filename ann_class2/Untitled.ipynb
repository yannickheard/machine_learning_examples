{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutorial is analogous to theano1.py\n",
    "# It introduces basic variables and functions\n",
    "# and shows how you can optimize a function.\n",
    "# I compare this to theano1.py multiple times.\n",
    "# So you might want to check that out first.\n",
    "\n",
    "# For the class Data Science: Practical Deep Learning Concepts in Theano and TensorFlow\n",
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# you have to specify the type\n",
    "A = tf.placeholder(tf.float32, shape=(5, 5), name='A')\n",
    "\n",
    "\n",
    "# but shape and name are optional\n",
    "v = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# I think this name is more appropriate than 'dot'\n",
    "w = tf.matmul(A, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.2450833]\n",
      " [-0.3730781]\n",
      " [ 1.3929094]\n",
      " [-0.6095692]\n",
      " [ 2.4458745]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# similar to Theano, you need to \"feed\" the variables values.\n",
    "# In TensorFlow you do the \"actual work\" in a \"session\".\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # the values are fed in via the appropriately named argument \"feed_dict\"\n",
    "    # v needs to be of shape=(5, 1) not just shape=(5,)\n",
    "    # it's more like \"real\" matrix multiplication\n",
    "    output = session.run(w, feed_dict={A: np.random.randn(5, 5), v: np.random.randn(5, 1)})\n",
    "\n",
    "    # what's this output that is returned by the session? let's print it\n",
    "    print(output, type(output))\n",
    "\n",
    "    # luckily, the output type is just a numpy array. back to safety!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[ 1.5826521  -0.80077946]\n",
      " [ 0.2737102  -0.3905468 ]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TensorFlow variables are like Theano shared variables.\n",
    "# But Theano variables are like TensorFlow placeholders.\n",
    "# Are you confused yet?\n",
    "\n",
    "# A tf variable can be initialized with a numpy array or a tf array\n",
    "# or more correctly, anything that can be turned into a tf tensor\n",
    "shape = (2, 2)\n",
    "x = tf.Variable(tf.random_normal(shape))\n",
    "# x = tf.Variable(np.random.randn(2, 2))\n",
    "t = tf.Variable(0) # a scalar\n",
    "\n",
    "# you need to \"initialize\" the variables first\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    out = session.run(init) # and then \"run\" the init operation\n",
    "    print(out) # it's just None\n",
    "\n",
    "    # eval() in tf is like get_value() in Theano\n",
    "    print(x.eval()) # the initial value of x\n",
    "    print(t.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, cost = 74.440, u = 6.800\n",
      "i = 1, cost = 9.390, u = 1.520\n",
      "i = 2, cost = -1.018, u = -0.592\n",
      "i = 3, cost = -2.683, u = -1.437\n",
      "i = 4, cost = -2.949, u = -1.775\n",
      "i = 5, cost = -2.992, u = -1.910\n",
      "i = 6, cost = -2.999, u = -1.964\n",
      "i = 7, cost = -3.000, u = -1.986\n",
      "i = 8, cost = -3.000, u = -1.994\n",
      "i = 9, cost = -3.000, u = -1.998\n",
      "i = 10, cost = -3.000, u = -1.999\n",
      "i = 11, cost = -3.000, u = -2.000\n",
      "i = 12, cost = -3.000, u = -2.000\n",
      "i = 13, cost = -3.000, u = -2.000\n",
      "i = 14, cost = -3.000, u = -2.000\n",
      "i = 15, cost = -3.000, u = -2.000\n",
      "i = 16, cost = -3.000, u = -2.000\n",
      "i = 17, cost = -3.000, u = -2.000\n",
      "i = 18, cost = -3.000, u = -2.000\n",
      "i = 19, cost = -3.000, u = -2.000\n",
      "i = 20, cost = -3.000, u = -2.000\n",
      "i = 21, cost = -3.000, u = -2.000\n",
      "i = 22, cost = -3.000, u = -2.000\n",
      "i = 23, cost = -3.000, u = -2.000\n",
      "i = 24, cost = -3.000, u = -2.000\n",
      "i = 25, cost = -3.000, u = -2.000\n",
      "i = 26, cost = -3.000, u = -2.000\n",
      "i = 27, cost = -3.000, u = -2.000\n",
      "i = 28, cost = -3.000, u = -2.000\n",
      "i = 29, cost = -3.000, u = -2.000\n",
      "i = 30, cost = -3.000, u = -2.000\n",
      "i = 31, cost = -3.000, u = -2.000\n",
      "i = 32, cost = -3.000, u = -2.000\n",
      "i = 33, cost = -3.000, u = -2.000\n",
      "i = 34, cost = -3.000, u = -2.000\n",
      "i = 35, cost = -3.000, u = -2.000\n",
      "i = 36, cost = -3.000, u = -2.000\n",
      "i = 37, cost = -3.000, u = -2.000\n",
      "i = 38, cost = -3.000, u = -2.000\n",
      "i = 39, cost = -3.000, u = -2.000\n",
      "i = 40, cost = -3.000, u = -2.000\n",
      "i = 41, cost = -3.000, u = -2.000\n",
      "i = 42, cost = -3.000, u = -2.000\n",
      "i = 43, cost = -3.000, u = -2.000\n"
     ]
    }
   ],
   "source": [
    "# let's now try to find the minimum of a simple cost function like we did in Theano\n",
    "u = tf.Variable(20.0)\n",
    "cost = u*u + (4*u) + 1.0\n",
    "\n",
    "# One difference between Theano and TensorFlow is that you don't write the updates\n",
    "# yourself in TensorFlow. You choose an optimizer that implements the algorithm you want.\n",
    "# 0.3 is the learning rate. Documentation lists the params.\n",
    "train_op = tf.train.GradientDescentOptimizer(0.3).minimize(cost)\n",
    "\n",
    "# let's run a session again\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "\n",
    "    # Strangely, while the weight update is automated, the loop itself is not.\n",
    "    # So we'll just call train_op until convergence.\n",
    "    # This is useful for us anyway since we want to track the cost function.\n",
    "    for i in range(44):\n",
    "        session.run(train_op)\n",
    "        print(\"i = %d, cost = %.3f, u = %.3f\" % (i, cost.eval(), u.eval()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n",
      "Looking for ../large_files/train.csv\n",
      "You have not downloaded the data and/or not placed the files in the correct location.\n",
      "Please get the data from: https://www.kaggle.com/c/digit-recognizer\n",
      "Place train.csv in the folder large_files adjacent to the class folder\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dda2a884bfcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-dda2a884bfcb>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# step 1: get the data and define all the usual variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_normalized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\machine_learning_examples\\ann_class2\\util.py\u001b[0m in \u001b[0;36mget_normalized_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Please get the data from: https://www.kaggle.com/c/digit-recognizer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Place train.csv in the folder large_files adjacent to the class folder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../large_files/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# 2-hidden layer NN in TensorFlow\n",
    "# This code is not optimized for speed.\n",
    "# It's just to get something working, using the principles we know.\n",
    "\n",
    "# For the class Data Science: Practical Deep Learning Concepts in Theano and TensorFlow\n",
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import get_normalized_data, y2indicator\n",
    "\n",
    "\n",
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "\n",
    "# copy this first part from theano2.py\n",
    "def main():\n",
    "    # step 1: get the data and define all the usual variables\n",
    "    Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "    max_iter = 15\n",
    "    print_period = 50\n",
    "\n",
    "    lr = 0.00004\n",
    "    reg = 0.01\n",
    "\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "    Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "    N, D = Xtrain.shape\n",
    "    batch_sz = 500\n",
    "    n_batches = N // batch_sz\n",
    "\n",
    "    # add an extra layer just for fun\n",
    "    M1 = 300\n",
    "    M2 = 100\n",
    "    K = 10\n",
    "    W1_init = np.random.randn(D, M1) / np.sqrt(D)\n",
    "    b1_init = np.zeros(M1)\n",
    "    W2_init = np.random.randn(M1, M2) / np.sqrt(M1)\n",
    "    b2_init = np.zeros(M2)\n",
    "    W3_init = np.random.randn(M2, K) / np.sqrt(M2)\n",
    "    b3_init = np.zeros(K)\n",
    "\n",
    "\n",
    "    # define variables and expressions\n",
    "    X = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "    T = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "    W1 = tf.Variable(W1_init.astype(np.float32))\n",
    "    b1 = tf.Variable(b1_init.astype(np.float32))\n",
    "    W2 = tf.Variable(W2_init.astype(np.float32))\n",
    "    b2 = tf.Variable(b2_init.astype(np.float32))\n",
    "    W3 = tf.Variable(W3_init.astype(np.float32))\n",
    "    b3 = tf.Variable(b3_init.astype(np.float32))\n",
    "\n",
    "    # define the model\n",
    "    Z1 = tf.nn.relu( tf.matmul(X, W1) + b1 )\n",
    "    Z2 = tf.nn.relu( tf.matmul(Z1, W2) + b2 )\n",
    "    Yish = tf.matmul(Z2, W3) + b3 # remember, the cost function does the softmaxing! weird, right?\n",
    "\n",
    "    # softmax_cross_entropy_with_logits take in the \"logits\"\n",
    "    # if you wanted to know the actual output of the neural net,\n",
    "    # you could pass \"Yish\" into tf.nn.softmax(logits)\n",
    "    cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Yish, labels=T))\n",
    "\n",
    "    # we choose the optimizer but don't implement the algorithm ourselves\n",
    "    # let's go with RMSprop, since we just learned about it.\n",
    "    # it includes momentum!\n",
    "    train_op = tf.train.RMSPropOptimizer(lr, decay=0.99, momentum=0.9).minimize(cost)\n",
    "\n",
    "    # we'll use this to calculate the error rate\n",
    "    predict_op = tf.argmax(Yish, 1)\n",
    "\n",
    "    costs = []\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "                Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "\n",
    "                session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})\n",
    "                if j % print_period == 0:\n",
    "                    test_cost = session.run(cost, feed_dict={X: Xtest, T: Ytest_ind})\n",
    "                    prediction = session.run(predict_op, feed_dict={X: Xtest})\n",
    "                    err = error_rate(prediction, Ytest)\n",
    "                    print(\"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, test_cost, err))\n",
    "                    costs.append(test_cost)\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    # increase max_iter and notice how the test cost starts to increase.\n",
    "    # are we overfitting by adding that extra layer?\n",
    "    # how would you add regularization to this model?\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\yanni\\anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17. 39.]\n",
      "421.0\n",
      "67.99000000000001\n",
      "11.508400000000002\n",
      "2.4713440000000007\n",
      "1.0254150400000002\n",
      "0.7940664064\n",
      "0.7570506250240001\n",
      "0.75112810000384\n",
      "0.7501804960006143\n",
      "0.7500288793600982\n",
      "0.7500046206976159\n",
      "0.7500007393116186\n",
      "0.750000118289859\n",
      "0.7500000189263775\n",
      "0.7500000030282203\n",
      "0.7500000004845152\n",
      "0.7500000000775223\n",
      "0.7500000000124035\n",
      "0.7500000000019845\n",
      "0.7500000000003176\n",
      "0.7500000000000506\n",
      "0.7500000000000082\n",
      "0.7500000000000013\n",
      "0.7500000000000001\n",
      "0.7500000000000001\n",
      "-0.4999999976919052\n"
     ]
    }
   ],
   "source": [
    "# Theano basics.\n",
    "# For the class Data Science: Practical Deep Learning Concepts in Theano and TensorFlow\n",
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import theano.tensor as T\n",
    "\n",
    "# just some different types of variables\n",
    "c = T.scalar('c')\n",
    "v = T.vector('v')\n",
    "A = T.matrix('A')\n",
    "\n",
    "\n",
    "# we can define a matrix multiplication\n",
    "w = A.dot(v)\n",
    "\n",
    "# how do these variables actually take on values?\n",
    "import theano\n",
    "\n",
    "matrix_times_vector = theano.function(inputs=[A, v], outputs=w)\n",
    "\n",
    "# let's import numpy so we can create real arrays\n",
    "import numpy as np\n",
    "A_val = np.array([[1,2], [3,4]])\n",
    "v_val = np.array([5,6])\n",
    "\n",
    "w_val = matrix_times_vector(A_val, v_val)\n",
    "print(w_val)\n",
    "\n",
    "# let's create a shared variable to we can do gradient descent\n",
    "# this adds another layer of complexity to the theano function\n",
    "\n",
    "x = theano.shared(20.0, 'x')\n",
    "\n",
    "# the first argument is its initial value, the second is its name\n",
    "\n",
    "# a cost function that has a minimum value\n",
    "cost = x*x + x + 1\n",
    "\n",
    "# in theano, you don't have to compute gradients yourself!\n",
    "x_update = x - 0.3*T.grad(cost, x)\n",
    "\n",
    "# x is not an \"input\", it's a thing you update\n",
    "# in later examples, data and labels would go into the inputs\n",
    "# and model params would go in the updates\n",
    "# updates takes in a list of tuples, each tuple has 2 things in it:\n",
    "# 1) the shared variable to update, 2) the update expression\n",
    "train = theano.function(inputs=[], outputs=cost, updates=[(x, x_update)])\n",
    "\n",
    "# write your own loop to call the training function.\n",
    "# it has no arguments!\n",
    "for i in range(25):\n",
    "    cost_val = train()\n",
    "    print(cost_val)\n",
    "\n",
    "# print the optimal value of x\n",
    "print(x.get_value())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\yanni\\appdata\\local\\pip\\cache\\wheels\\33\\e0\\86\\12647586a15bd29c062c9996231380908fb2dcf6a5df1c6f84\\theano-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\yanni\\anaconda3\\lib\\site-packages (from theano) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\yanni\\anaconda3\\lib\\site-packages (from theano) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\yanni\\anaconda3\\lib\\site-packages (from theano) (1.14.0)\n",
      "Installing collected packages: theano\n",
      "Successfully installed theano-1.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
